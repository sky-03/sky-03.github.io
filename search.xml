<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CTF新手指南]]></title>
    <url>%2F2017%2F09%2F30%2FCTFinfo%2F</url>
    <content type="text"><![CDATA[Knowing is not enough; we must apply. Willing is not enough; we must do. CTF新手指南12“Knowing is not enough; we must apply. Willing is not enough; we must do.” （仅仅知道还不够，我们必须付诸实践。仅有意愿还不够，我们必须付诸行动。）—— Johann Wolfgang von Goethe CTF（夺旗赛）介绍CTF(Capture The Flag)中文一般译为夺旗赛，在网络安全领域中指的是网络安全技术人员之间进行技术竞技的一种比赛形式。CTF起源于1996年DEFCON全球黑客大赛，以代替之前黑客们通过互相发起真实攻击进行技术比拼的方式。 CTF（比赛）WEB1涉及到常见的Web漏洞，诸如注入、XSS、文件包含、代码执行、上传等漏洞. Crypto1即密码学，题目考察各种加解密技术，包括古典加密技术、现代加密技术甚至出题者自创加密技术，主要考查参赛选手密码学相关知识点. MISC1即安全杂项，题目涉及流量分析、电子取证、人肉搜索、数据分析、大数据统计等等，覆盖面比较广，主要考查参赛选手的各种基础综合知识. Reverse1即逆向工程，题目涉及到软件逆向、破解技术等，要求有较强的反汇编、反编译扎实功底。主要考查参赛选手的逆向分析能力. STEGA1即隐写术，题目的Flag会隐藏到图片、音频、视频等各类数据载体中供参赛选手获取。主要考查参赛选手的对各种隐写工具、隐写算法的熟悉程度. PPC1即编程类题目，题目涉及到程序编写、编程算法实现，当然PPC相比ACM来说，还是较为容易的。至于编程语言嘛，推荐使用Python或Ruby来尝试 PWN1在黑客俚语中代表着攻破，取得权限，在CTF比赛中它代表着溢出类的题目，其中常见类型溢出漏洞有栈溢出、堆溢出。主要考察参数选手对漏洞的利用能力. 以上总结来源于实验吧]]></content>
      <categories>
        <category>CTF</category>
      </categories>
      <tags>
        <tag>CTF</tag>
        <tag>sky</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络爬虫前奏笔记]]></title>
    <url>%2F2017%2F09%2F30%2F%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%89%8D%E5%A5%8F%2F</url>
    <content type="text"><![CDATA[简单的网络爬虫学习笔记 网络爬虫前奏笔记准备工作Requests[自动爬去HTML页面自动网络请求提交]robots.txt[网咯爬虫排除标准]Beautiful Soup[解析HTML页面]Projects[实战项目A/B]Re[正则表达式详解提取页面关键信息]Scrapy*[网络爬虫原理介绍]Requests库入门Response对象的属性|属性|说明|—-|—-|r.status_code|HTTP请求的返回状态，200表示连接成功，404表示失败|r.text|HTTP响应内容的字符串形式，即，url对应的页面内容|r.encoding|从HTTP header中猜测的响应内容编码方式|r.apparent_encoding|从内容中分析出的响应内容编码方式(备选编码方式)|r.content|HTTP响应内容的二进制形式 Requests库的异常 异常 说明 requests.ConnectionError 网络连接错误异常，如DNS查询失败、拒绝连接等 requests.HTTPError HTTP错误异常 requests.URLRequired URL缺失异常 requests.TooManyRedirects 超过最大重定向次数，产生重定向异常 requests.ConnectTimeout 连接远程服务器超时异常 requests.Timeout 请求URL超时，产生异常 r.raise_for_status() 如果不是200，产生异常requests.HTTPError Reequests的7个主要方法 方法 说明 requests.requests() 构造一个请求，以支撑下面的方法 requests.get() 获取HTML网页的主要方法，对应于HTTP的GET requests.head() 获取HTML网页头信息的方法，对应于HTTP的HEAD requests.post() 向HTML网页提交POST请求的方法，对应于HTTP的POST requests.put() 向HTML提交PUT请求方法，对应于HTTP的PUT requests.patch() 向HTML网页提交局部修改请求，对已于HTTP的PATCH requests.delete() 向HTML页面提交删除请求，对应于HTTP的DELETE HTTP协议对资源的操作 Requests库主要方法解析requests.request(method,url,**kwargs) method:请求方式url:拟获取页面的url链接**kwargs:控制访问的参数，共13个 method：请求方式r = requests.request(‘GET’,url,kwargs)r = requests.request(‘HEAD’,url,kwargs)r = requests.request(‘POST’,url,kwargs)r = requests.request(‘PUT’,url,kwargs)r = requests.request(‘PATCH’,url,kwargs)r = requests.request(‘DELETE’,url,kwargs)r = requests.request(‘OPTIONS’,url,**kwargs) **kwargs:控制访问的参数，均为可选项params:字典或字节序列，作为参数增加到url中1234kv = &#123;'key1':'value1','key2':'value2'&#125;r = requests.request('GET','http://python123.io/ws',params=kv)print(r.url)http://python123.io/ws?key1=value1&amp;key2=value2 data:字典、字节序列或文件对象，作为Request的内容12kv = &#123;'key1':'value1','key2':'value2'&#125;r = requests.request('POST','http://python123.io/ws',data=kv) json:JSON格式的数据，作为Request的内容12kv = &#123;'key1':'value1'&#125;r = requests.request('POST','http://python123.io/ws',json=kv) headers:字典，HTTP定制头12hd = &#123;'user-agent':'Chrome/10'&#125;r = requests.request('POST','http://python123.io/ws',headers=hd) cookies:字典或CookieJar，Request中的cookieauth:元祖，支持HTTP认证功能files:字典类型，传输文件12fs = &#123;'files':open('data.xls','rb')&#125;r = requests.request('POST','http://python123.io/ws',files=fs) timeout:设定超时时间，秒为单位1r = requests.request('GET','http://www.baidu.com',timeout=10) proxies:字典类型，设定访问代理服务器，可以增加登录认证123pxs = &#123;'http':http://user:pass@10.10.10.1:1234'https:'https://10.10.10.1:4321'&#125;r = requests.requet('GET','http://www.baidu.com',proxies=pxs) allow_redirects:True/False,默认为True,重定向开关stream:True/False,默认为True，获取内容立即下载开关verify:True/false,默认为True，认证SSL证书开关 requests.get(url,params=None,**kwargs)[同request方法] requests.head(url,**kwargs)[同request方法] requests.post(url,data=None,json=None,**keargs)[同request方法] requests.put(url,data=None,**kwargs)requests.patch(url,data=None,**kwargs)requests.delete(url,*kwargs)requests.get(url,params=None,**kwargs)[最常用的方法]requests库入门小结requests.get()requests.head()爬取网页的通用代码框架1234567try: r = requests.get(url,timeout=30) r.raise_for_status() r.encoding = r.apparent_encoding return r.textexcept: return "产生异常" 第二单元第一种：小规模 数据量小 爬取速度不敏感 Requestsk库 —&gt; 爬去网页 玩转网页第二种： 中规模，数据模块较大爬取速度敏感 Scrapy库 —&gt; 爬取网站 爬取系列网站第三种：大规模，搜索引擎爬取速度关键 定制开发 —–&gt;爬取全网 网络爬虫的限制1.判断User-Agent进行限制2.Robots协议 Robots协议Robots Exclusion Sandard 网络爬虫排除标准robots.txt语法:User-Agent： [代表所有]Disallow： / [/代表根目录] Requests库爬取实例京东12345678910#-*- coding: UTF-8 -*- import requestsurl = "https://item.jd.com/2967929.html"try: r = requests.get(url) r.raise_for_status() r.encoding = r.apparent_encoding print(r.text[:1000])except: print("爬取失败") [ #-- coding: UTF-8 -- 是为了解决这个python2会默认使用ASCII编码] 亚马逊 百度 360搜索引擎自己运行的结果图：python代码 网络图片的爬取与存储 实例五 IP地址归属底地查询 Beautiful Soup12from bs4 import BeautifulSoupsoup = BeautifulSoup(&lt;p&gt;data&lt;/p&gt;,"html.parser") 终端运行代码如下：1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get("http://python123.io/ws/demo.html")&gt;&gt;&gt; r.textu'&lt;html&gt;&lt;head&gt;&lt;title&gt;This is a python demo page&lt;/title&gt;&lt;/head&gt;\r\n&lt;body&gt;\r\n&lt;p class="title"&gt;&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;&lt;/p&gt;\r\n&lt;p class="course"&gt;Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n&lt;a href="http://www.icourse163.org/course/BIT-268001" class="py1" id="link1"&gt;Basic Python&lt;/a&gt; and &lt;a href="http://www.icourse163.org/course/BIT-1001870001" class="py2" id="link2"&gt;Advanced Python&lt;/a&gt;.&lt;/p&gt;\r\n&lt;/body&gt;&lt;/html&gt;'&gt;&gt;&gt; demo = r.text&gt;&gt;&gt; from bs4 import BeautifulSoup&gt;&gt;&gt; soup = BeautifulSoup(demo, "html.parser")&gt;&gt;&gt; print(soup.prettify())&lt;html&gt; &lt;head&gt; &lt;title&gt; This is a python demo page &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="title"&gt; &lt;b&gt; The demo python introduces several python courses. &lt;/b&gt; &lt;/p&gt; &lt;p class="course"&gt; Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses: &lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt; Basic Python &lt;/a&gt; and &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt; Advanced Python &lt;/a&gt; . &lt;/p&gt; &lt;/body&gt;&lt;/html&gt;&gt;&gt;&gt; Beautiful Soup库的理解Beautiful Soup库是解析、遍历、维护、“标签书”的功能库引用方式：from bs4 imiport BeautifulSoup Python大小写敏感 基于bs4的使用案例标签树的下行遍历|属性|说明|—-|—-| .contents | 子节点的列表，将\所有的儿子节点存入列表| .children | 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点| .descendants | 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历标签树的上行遍历|属性|说明|—|—| .parent | 节点的父亲标签| .parents | 节点先辈标签的迭代类型，用于循环遍历先辈节点标签树的上行遍历代码123456soup = BeautifulSoup(demo,"html.parser")for parent in soup.a.parents: if parent is None: print(parent) else: print(parent.name) 标签树的平行bianli|属性|说明||—-|—-|.next_sibling|返回按照HTML文本顺序的下一个平行节点标签|.previous_sibling|返回按照HTML文本顺序的上一个平行节点标签|.next_siblings|迭代类型，返回按照类型HTML文本顺序的后续所有平行节点标签|.previous_siblings|迭代类型，返回按照类型HTML文本顺序的前序所有平行节点标签平行遍历的条件：平行遍历发生在同意父节点下的个节点时间标签树的平行遍历12for sibling in soup.a.next_sibling: print(sibling) #遍历后续节点 12for sibling in soup.a.previous_siblings: print(sibling) #遍历前续节点 bs4库的prettify()方法美化输出页面 Beautiful Soup小结 信息组织与提取方法信息的标记xml json yamlXML12&lt;name&gt;....&lt;/name&gt;&lt;!--- ----&gt; JSON123“key”:“value”“key”:[“value1”,“value2”]“key”:[“subkey”:“subvalue”] YAML123456key : valuekey : # Comment-value1 ##-表示并行关系-value2key : subkey : subvalue XML实例12345678910&lt;person&gt; &lt;firstName&gt;Trian&lt;/firstName&gt; &lt;lastName&gt;Song&lt;/lastName&gt; &lt;adress&gt; &lt;streetAddr&gt;中关村南大街5号&lt;/sreetAddr&gt; &lt;city&gt;北京市&lt;/city&gt; &lt;zipcode&gt;100081&lt;/zipcode&gt; &lt;/address&gt; &lt;prof&gt;Computer System&lt;/prof&gt;&lt;prof&gt;Security&lt;/prof&gt;&lt;/person&gt; JSON实例YAML实例 三种信息标记形式的比较 标记语言 区别 应用地方 XML 最早的通用信息标记语言，可扩展性好，但繁琐 internet JSON 信息有类型，适合程序处理（js），较XML简洁 移动应用云端和节点 YAML 信息无类型，文本信息比列最高，可读性好 各类系统的配置文件 信息提取的一般方法方法一:完整解析信息的标记形式，在提取关键信息。XML JSON YAML需要标记解析器 例如：bs4库的标签树遍历优点：信息解析准确缺点：提取过程繁琐，速度慢方法二：无视标记信息，直接搜索关键信息搜索对信息的文本查找函数即可。优点：提取过程简洁，速度较快缺点：提取结果准确性与信息内容无关 实例提取HTML页面的url链接思路：1）搜索到所有标签 2）解析标签格式，提取href后的链接内容12345&gt;&gt;&gt; for link in soup.find_all('a'):... print(link.get('href'))...http://www.icourse163.org/course/BIT-268001http://www.icourse163.org/course/BIT-1001870001 .find_all()的用法12&gt;&gt;&gt; soup.find_all(['a','b'])[&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;, &lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;] 123456789101112&gt;&gt;&gt; for tag in soup.find_all(True):... print(tag.name)...htmlheadtitlebodypbpaa 123456&gt;&gt;&gt; import re&gt;&gt;&gt; for tag in soup.find_all(re.compile('b')):... print(tag.name)...bodyb 12&gt;&gt;&gt; soup.find_all('p','course')[&lt;p class="course"&gt;Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt; and &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;.&lt;/p&gt;] 12&gt;&gt;&gt; soup.find_all(id='link1')[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;] 12&gt;&gt;&gt; soup.find_all(id=re.compile('link'))[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;] 12345&gt;&gt;&gt; soup.find_all('a')[&lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt;Basic Python&lt;/a&gt;, &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt;Advanced Python&lt;/a&gt;]&gt;&gt;&gt; soup.find_all('a',recursive=False)[] 12345&gt;&gt;&gt; soup.find_all(string = "Basic Python")[u'Basic Python']&gt;&gt;&gt; soup.find_all(string = re.compile("Python"))[u'Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n', u'Basic Python', u'Advanced Python'] &lt;tag&gt;(..)等价与&lt;tag&gt;.find_all(...)soup(..)等价与 soup.find_all(...)扩展方法 中国大学排名定向爬虫步骤一：从网络上获取大学排名网页内容 getHTML()步骤二：提取网页内容中信息到合适的数据结构 fillUnivList()步骤三：利用数据结构展示并输出结果 printUnivList()采用二维列表显示 代码123456789101112131415161718192021222324252627282930import requestsfrom bs4 import BeautifulSoupimport bs4def getHTMLText(url): try: r = requests.get(url,timeout = 30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return ""def fillUnivList(ulist,html): soup = BeautifulSoup(html,"html.parser") for tr in soup.find('tbody').children: if isinstance(tr,bs4.element.Tag): tds = tr('td') ulist.append([tds[0].string,tds[1].string,tds[2].string])def printUnivList(ulist,num): tplt = "&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;" print(tplt.format("排名","学校名称","总分"),chr(12288)) for i in range(num): u = ulist[i] print("&#123;:^10&#125;\t&#123;:^6&#125;\t&#123;:^10&#125;".format(u[0],u[1],u[2]),chr(12288))def main(): uinfo = [] url = 'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html' html = getHTMLText(url); fillUnivList(uinfo,html) printUnivList(uinfo,20) # 20 unive 中文输出对齐：char(12288) 第三单元正则表达式正则表达式是用来简介表示一组字符串的表达式通用的字符串表达框架简洁表达一组字符串的表达式针对字符串表达“简洁”和“特征”思想的工具表达文本类型的特征（病毒、入侵等）同时查找或替换一组字符串匹配字符串的全部或部分主要使用在：字符串匹配中 正则表达式语法正则表达式是由字符和操作符构成的 RE库的使用RE库的主要功能函数 regex = re.compile(pattern,flags=0)将正则表达式的字符串形式编译成正则表达式对象 Match对象的属性 Re库的贪婪匹配和最小匹配 淘宝商品比价定向爬虫步骤一：提交商品搜索请求，循环获取页面步骤二：对于每个页面，提取商品名称和价格信息步骤三：将信息输出到屏幕上。 股票网站定向爬取步骤一：从东方财富网获取股票列表步骤二：根据股票列表逐个到百度股票获取个股信息步骤三：将结果存储到文件 Scrapy爬虫框架 requests vs Scrapy scrapy常用命令 scrapy常用操作 Requests类class scrapy.http.Request()Requests对象表示一个http请求有spider生成，有download执行Response类class scrapy.http.Response()Response对象表示一个http响应由Downlaoder生成，由Spider处理Item类 class scrapy.item.Item()Item对象表示一个从HTML页面中提取的信息内容有Spider生成，有Item Pipeline处理Item类似字典类型，并不是完全的字典类型Scrapy爬虫提取信息的方法CSS selector .css(‘a::attr(href)’).extract()Scarpy配置并发并连接选项]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>other</tag>
      </tags>
  </entry>
</search>
