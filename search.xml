<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《python开发与项目实战》学习笔记]]></title>
    <url>%2F2018%2F01%2F28%2Fpython%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E4%B8%8E%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[未写完，待更新. python爬虫开发与项目实站回顾python编程IO编程文件读写open函数的mode参数|值|功能描述||–|——–|’r’|读模式|’w’|写模式|’a’|追加模式|’b’|二进制模式(可添加到其他模式使用)|’+’|读写模式(可添加到其他模式使用) 操作文件和目录python中对文件和目录的操作经常用到os模块和shutil模块shutil模块可以复制文件夹、复制文件、移动文件、删除目录其他的对操作文件和目录的操作用的是os模块 序列化操作序列化：把内存中的变量变成可存储或可传输的过程在python中提供两个模块：cPickle和pickle来实现序列化，前者是由C语言编写的，效率比后者高很多，但是两个模块的功能是一样的。现在流行的是序列化为JSON格式1234try: import cPickle as pickleexcept: import pickle 进程和线程多进程python实现多进程的主要方式有两种，一种方法是使用os模块中的fork方法，另一种是方法是使用multiprocessing模块1.使用os模块中的fork方式实现多进程fork方法调用一次，返回两次，原因在于操作系统将当前进程(父进程)复制出一份进程(子进程)。子进程中永远返回0，父进程中返回的是子进程的ID其中os模块的getpid方法用于获取当前进程的ID,getppid方法用于获取父进程的ID。2.使用multiprocessing模块创建进程以上两种方法只适用于创建少量的子进程，要启动大量的子进程，可以用进程池Pool3.multiprocessing模块提供一个Pool类来代表进程池对象Pool对象调用join()方法会等待所有子进程执行完毕，调用jion()之前必须先调用close()，调用close()之后就不能继续添加新的Process了4.进程通信python提供了多种进程间通信的方式，列入：Queue、Pipe、Value+Array等。主要讲解Queue和Pipe这两种方式。这两个区别在于：Pipe常用于两个进程间通信，Queue用来在多个进程间实现通信。Queue通信方式。Queue是多进程安全的队列，可以使用Queue实现多进程之间的数据传递。有两个方法：Put和Get可以进行Queue操作：Pipe通信机制，Pipe常用来在两个进程进行通信，两个进程分别位于管道的两端。 多线程多线程类似于同时执行多个不同程序，多线程运行有如下的优点：1.可以把运行时间长的任务放到后台能去处理。2.程序运行速度更快3.用户界面可以更加吸引人python提供了两个模块：thread和threading,thread是低级模块，threading是高级模块。 用threading模块创建多线程第一种方式是把一个函数传入并创建Thread实例，然后调用start方法开始执行。第二种欧冠方式从threading.Thread继承创建线程类。 线程同步使用Thread对象的Lock和RLock可以实现简单的线程同步，这两个对象都有acquire和release方法RLock对象允许一个线程多次对其进行acquire操作，因为其内部通过一个counter变量维护着线程acquire的次数，而且每一次的acquire操作必须有一个release操作与之对象，在所有的release操作完成之后，别的线程才能申请RLock队象。 全局解释器锁(GIL) 协程分布式进程网络编程1.socket类型2.Socket函数 TCP编程UDP编程WEB前端基础初识网络爬虫爬虫概述略 HTTP请求的python实现urllib2/urllib 实现httplib/urllib 实现更人性化的requests一个完整的请求与响应模板以Get方式123import requestsr = reqests.get('http://www.baidu.com')print r.content r.content返回的是字节形式以post方式1234import requestspostdata = &#123;'key':'value'&#125;r = requests.post('http://www.xxxx.com/login',data = postdata)print r.content 1234import requests payload = &#123;'Keywords':'blog:qiyeboy','pageindex':1&#125;r = requests.get('http://zzk,cnblogs.com/s/blogpost', params = payload)print r.url 响应与编码123456import requestsr = requests.get('http://www.baidu.com')print r.contentprint r.encodingr.encoding = 'utf-8'print r.text 编码：chardet12345import requestsr = requests.get('http://www.baidu.com')print chartdet.detect(r.content)r.encoding = chardet.detect(r.content)['encoding']print r.text 请求头的处理 响应码code和响应头headers处理 Cookie 处理 重定向与历史消息 超时设置1requests.get('http://github.com',timeout = 20) 代理设置 HTML解析大法初识FirebugRE正则基本语法的使用入门小例子常用元字符 字符转义如果你想查找元字符本身的话，因为它们具有特定功能，没办法把它们指定为普通字符。这个时候就需要用到转义，使用“\”来取消这些字符的特殊意义。 重复 字符集合正则表达式是通过[]来实现自定义字符集合，[abcd]就是匹配abcd中的任意一个字符，[.,!]匹配标点符号 分支条件如果满足其中任意一种规则都应该当成匹配，具体方法是用“|”把不同的规则分隔开。 分组IP地址的匹配：((25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d){3}(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)) 反义 后向引向 零宽断言零宽断言共有四种形式:前两种形式是正向零宽断言，后两种是负向零宽断言正向零宽断言的两种形式：负向零宽断言的两种形式： 注释 贪婪与懒惰 处理选项 python与正则python通过re模块提供对正则表达式的支持。使用re的一般步骤是：1.先将正则表达的字符串形式编译为Pattern实例2.然后使用Pattern实例处理文本并获得匹配结果3.最后使用Match实例获得信息，进行其他操作主要方法：参数flag是匹配模式，取值可以使用按位或运算符”|”表示同时生效。flag的可选值如下： re.match(pattern,string[,flags])总结：re.match()方法从开头开始匹配，若开头没有匹配成功，则返回none，反之获取匹配的结果 re.search(pattern,string[,flags])总结：search()会扫描整个string查找匹配 re.split(pattern,string[,maxsplit]) re.findall(pattern,string[,flags]) re.finditer(pattern,string[,flags]) re.sub(pattern,repl,string[,count]) re.subn(pattern,repl,string[,count])Match对象的属性和方法函数调用的方式 强大的Beautiful Soup解析器比较 Beautiful Soup的使用快速开始 对象种类共有4种对象：1.Tag2.NavigableString3.BeautifulSoup4.Comment 遍历子文档树null 搜索文档树null CSS选择器null lxml的XPath解析数据存储hTML正文抽取存储为JSON见daomu.py 存储为CSV见daomu1.py 多媒体文件抽取见duomeiti.py Email提醒见Email.py 实战项目：基础爬虫基础爬虫架构及运行原理基础爬虫:功能简单，仅考虑功能的实现，未涉及优化和稳健性的考虑。基础爬虫框架:主要包括五大模块，分别为爬虫调度器、URL管理器、HTML下载器、HTML解析器、数据存储器。 URL管理器主要包括两个变量:一个是已爬取URL的集合，另一个是未爬取URL的集合。pythonset类型，主要是使用set的去重功能，防止链接重复爬取，因为爬取链接重复时容易造成死循环。解决方案大概有三种:1.内存去重2.关系数据库去重3.缓存数据库去重爬取数量较大时，缓存数据库去重，爬取数量较小时采用内存去重代码见：URLManager.py HTML下载器见HTMLDownload.py HTML解析器见HtmlParser.py 数据存储器见DataOutput.py 爬虫调度器见SpiderMan.py所有文件在firstSpider文件下 简单分布式爬虫简单分布式爬虫结构 数据库存储讲解了python对SQLite、MySQL、MongoDB的操作 动态网站抓取Ajax和动态html了解Ajax和动态html的定义和原理 动态爬虫1：爬取影评信息略 PhantomJSPhantomJS是一个基于Webkit的服务器端JavaScript API.它支持web而无需浏览器支持，不仅运行快，原生支持各种标准:DOM处理、CSS选择器、JSON、Canvas，和SVG。 SeleniumSelenium是一个自动化测试工具，支持各种浏览器，包括Chrome、Safari、Firebox等 爬取去哪网略 web端协议分析10.1 网页登录POST分析登录之后才能进行页面爬取的情况，属于深层次的网页爬取 隐藏表单分析加密数据分析]]></content>
      <categories>
        <category>Other</category>
      </categories>
      <tags>
        <tag>Other</tag>
        <tag>PWBIn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CTF新手指南]]></title>
    <url>%2F2017%2F09%2F30%2FCTFinfo%2F</url>
    <content type="text"><![CDATA[Knowing is not enough; we must apply. Willing is not enough; we must do. CTF新手指南12“Knowing is not enough; we must apply. Willing is not enough; we must do.” （仅仅知道还不够，我们必须付诸实践。仅有意愿还不够，我们必须付诸行动。）—— Johann Wolfgang von Goethe CTF（夺旗赛）介绍CTF(Capture The Flag)中文一般译为夺旗赛，在网络安全领域中指的是网络安全技术人员之间进行技术竞技的一种比赛形式。CTF起源于1996年DEFCON全球黑客大赛，以代替之前黑客们通过互相发起真实攻击进行技术比拼的方式。 CTF（比赛）WEB1涉及到常见的Web漏洞，诸如注入、XSS、文件包含、代码执行、上传等漏洞. Crypto1即密码学，题目考察各种加解密技术，包括古典加密技术、现代加密技术甚至出题者自创加密技术，主要考查参赛选手密码学相关知识点. MISC1即安全杂项，题目涉及流量分析、电子取证、人肉搜索、数据分析、大数据统计等等，覆盖面比较广，主要考查参赛选手的各种基础综合知识. Reverse1即逆向工程，题目涉及到软件逆向、破解技术等，要求有较强的反汇编、反编译扎实功底。主要考查参赛选手的逆向分析能力. STEGA1即隐写术，题目的Flag会隐藏到图片、音频、视频等各类数据载体中供参赛选手获取。主要考查参赛选手的对各种隐写工具、隐写算法的熟悉程度. PPC1即编程类题目，题目涉及到程序编写、编程算法实现，当然PPC相比ACM来说，还是较为容易的。至于编程语言嘛，推荐使用Python或Ruby来尝试 PWN1在黑客俚语中代表着攻破，取得权限，在CTF比赛中它代表着溢出类的题目，其中常见类型溢出漏洞有栈溢出、堆溢出。主要考察参数选手对漏洞的利用能力. 以上总结来源于实验吧]]></content>
      <categories>
        <category>CTF</category>
      </categories>
      <tags>
        <tag>CTF</tag>
        <tag>sky</tag>
      </tags>
  </entry>
</search>
